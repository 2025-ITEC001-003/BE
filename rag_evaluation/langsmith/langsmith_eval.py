import os
import sys

# ì ˆëŒ€ ê²½ë¡œ ê¸°ë°˜ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
rag_eval_dir = os.path.dirname(current_dir)
project_root = os.path.dirname(rag_eval_dir)

# src ëª¨ë“ˆ ë¡œë“œë¥¼ ìœ„í•´ project_rootë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, project_root)

from dotenv import load_dotenv
from langchain.smith import RunEvalConfig, run_on_dataset
from langsmith import Client
from langchain_core.prompts import load_prompt
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

from src.core import get_compression_retriever, llm_default

load_dotenv()

# 1. ì„¤ì • ë° ë¦¬ì†ŒìŠ¤ ë¡œë“œ
# í‰ê°€í•  ë°ì´í„°ì…‹ ì´ë¦„ (upload_dataset.pyì—ì„œ ì§€ì •í•œ ì´ë¦„ê³¼ ë™ì¼í•´ì•¼ í•¨)
DATASET_NAME = "Jeju_Tourism_QA_Set_KO"

PROMPT_FILE = os.path.join(project_root, "prompts", "jeju_tourism_rag_prompt.yaml")
try:
    rag_prompt = load_prompt(PROMPT_FILE)
    print(f"âœ… í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ: {PROMPT_FILE}")
except Exception as e:
    print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ({e})")
    from langchain_core.prompts import ChatPromptTemplate
    rag_prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {question}"""
    )

retriever = get_compression_retriever()

# 2. í‰ê°€ìš© RAG ì²´ì¸ êµ¬ì„± (Context ë°˜í™˜ í•„ìˆ˜)
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def evaluation_target_chain(inputs):
    """
    LangSmith í‰ê°€ë¥¼ ìœ„í•œ RAG ì²´ì¸ ë˜í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    context_qa í‰ê°€ë¥¼ ìœ„í•´ 'answer'ì™€ í•¨ê»˜ 'contexts'ë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    question = inputs["question"]
    
    # 1. ê²€ìƒ‰ (Retrieval)
    docs = retriever.invoke(question)
    formatted_context = format_docs(docs)
    
    # 2. ë‹µë³€ ìƒì„± (Generation)
    chain = (
        rag_prompt 
        | llm_default 
        | StrOutputParser()
    )
    
    answer = chain.invoke({
        "context": formatted_context,
        "question": question
    })
    
    # 3. ê²°ê³¼ ë°˜í™˜ (ì¤‘ìš”: contexts í‚¤ í¬í•¨)
    return {
        "answer": answer,           # ìƒì„±ëœ ë‹µë³€
        "contexts": [d.page_content for d in docs], # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ë¦¬ìŠ¤íŠ¸ (context_qaìš©)
        "retrieved_docs": docs      # (ì„ íƒ) ë©”íƒ€ë°ì´í„° í¬í•¨ ì›ë³¸ ë¬¸ì„œ
    }

# 3. LangSmith í‰ê°€ ì„¤ì •
def run_evaluation():
    client = Client()
    
    # í‰ê°€ì(Judge) ëª¨ë¸ ì„¤ì • - ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ gpt-4o ê¶Œì¥
    eval_llm = ChatOpenAI(
        model="gpt-4o", 
        temperature=0
    )

    # í‰ê°€ ì§€í‘œ ì„¤ì •
    eval_config = RunEvalConfig(
        evaluators=[
            # 1. QA (Correctness): ì •ë‹µ(Ground Truth)ê³¼ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ í‰ê°€
            "qa", 
            
            # 2. CoT QA (Chain of Thought): ì´ìœ ë¥¼ ë¨¼ì € ìƒê°í•˜ê³  ì±„ì  (ë” ì •í™•í•¨)
            "cot_qa",
            
            # 3. Context QA (Context Relevance): 
            # ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ(Context)ì— ê¸°ë°˜í–ˆëŠ”ì§€ í‰ê°€ (Hallucination ì²´í¬)
            "context_qa", 
        ],
        eval_llm=eval_llm,
        # ì˜ˆì¸¡ê°’ê³¼ ì°¸ì¡°ê°’(ì •ë‹µ)ì˜ í‚¤ ë§¤í•‘
        prediction_key="answer",  
        reference_key="answer",   # ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì‹œ ground_truthë¥¼ 'answer'ë¡œ ë§¤í•‘í–ˆëŠ”ì§€ í™•ì¸ í•„ìš”
        input_key="question"      # ë°ì´í„°ì…‹ì˜ ì§ˆë¬¸ ì»¬ëŸ¼
    )

    print(f"ğŸš€ LangSmith í‰ê°€ ì‹œì‘: {DATASET_NAME}")
    print(f"   - Evaluators: qa, cot_qa, context_qa")
    
    try:
        results = run_on_dataset(
            client=client,
            dataset_name=DATASET_NAME,
            llm_or_chain_factory=evaluation_target_chain,
            evaluation=eval_config,
            project_name="jeju-rag-eval-experiment-v1", # ì‹¤í—˜ í”„ë¡œì íŠ¸ ì´ë¦„ (ë²„ì „ ê´€ë¦¬ìš©)
        )
        print("âœ… í‰ê°€ ì™„ë£Œ! LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print(f"ğŸ”— í”„ë¡œì íŠ¸ ë§í¬: {results['project_url'] if 'project_url' in results else 'N/A'}")
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("íŒ: ë°ì´í„°ì…‹ ì´ë¦„ì´ ì •í™•í•œì§€, LangSmith API Keyê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    run_evaluation()
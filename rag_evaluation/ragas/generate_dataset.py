import os
import glob
import pandas as pd
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from ragas.testset import TestsetGenerator
from ragas.testset.synthesizers.single_hop.specific import SingleHopSpecificQuerySynthesizer
from ragas.testset.synthesizers.multi_hop.specific import MultiHopSpecificQuerySynthesizer
from ragas.testset.synthesizers.multi_hop.abstract import MultiHopAbstractQuerySynthesizer
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_core.rate_limiters import InMemoryRateLimiter
from src.core import get_cached_embedder

load_dotenv()

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR)) 
DATA_DIR = os.path.join(PROJECT_ROOT, "data", "processed_md_originals")
OUTPUT_FILE_PATH = os.path.join(PROJECT_ROOT, "rag_evaluation/dataset", "english_testset.csv")
TEST_SIZE = 30

# Rate Limit & Retry ì ìš©ëœ LLM ìƒì„±
rate_limiter = InMemoryRateLimiter(
    requests_per_second=2.0,      # ì´ˆë‹¹ ìš”ì²­ 2íšŒ ì •ë„
    check_every_n_seconds=0.1,
    max_bucket_size=4            # ì•½ê°„ì˜ ë²„ìŠ¤íŠ¸ í—ˆìš©
)

generator_llm = ChatOpenAI(
    model="gpt-5.1",
    temperature=0,
    timeout=60,
    max_retries=5,   # LLM ë‚´ë¶€ ì¬ì‹œë„
    response_format={"type": "json_object"},
    rate_limiter=rate_limiter
)

generator_llm_wrapper = LangchainLLMWrapper(generator_llm)
ragas_embeddings = LangchainEmbeddingsWrapper(get_cached_embedder())

# Markdown íŒŒì¼ ë¡œë“œ
def load_raw_markdown_files():
    print(f"ğŸ“‚ ì›ë³¸ ë°ì´í„° í´ë” íƒìƒ‰: {DATA_DIR}")

    if not os.path.exists(DATA_DIR):
        print(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DATA_DIR}")
        return []

    md_files = glob.glob(os.path.join(DATA_DIR, "**/*.md"), recursive=True)

    if not md_files:
        print(f"âŒ '{DATA_DIR}' ê²½ë¡œì—ì„œ .md íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    print(f"   -> ì´ {len(md_files)}ê°œì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë°œê²¬")

    documents = []
    for file_path in md_files:
        print(f"   - ë¡œë“œ ì¤‘: {os.path.basename(file_path)}")
        try:
            loader = UnstructuredMarkdownLoader(file_path)
            docs = loader.load()

            for doc in docs:
                doc.metadata["filename"] = os.path.basename(file_path)

            documents.extend(docs)

        except Exception as e:
            print(f"   âš ï¸ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")

    return documents

# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ (í•µì‹¬)
def generate_with_retry(generator, documents, test_size, query_distribution, max_attempts=5):
    for attempt in range(1, max_attempts + 1):
        print(f"\n=============================")
        print(f"  ğŸ” í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹œë„ {attempt}/{max_attempts}")
        print(f"=============================\n")

        try:
            testset = generator.generate_with_langchain_docs(
                documents=documents,
                testset_size=test_size,
                query_distribution=query_distribution,
                raise_exceptions=False
            )

            df = testset.to_pandas()

            # ì¶©ë¶„íˆ ìƒì„±ë˜ë©´ ì„±ê³µ
            if len(df) >= test_size:
                print(f"ğŸ‰ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë¨: {len(df)}ê°œ")
                return df

            print(f"âš ï¸ {len(df)}ê°œë°–ì— ìƒì„±ë˜ì§€ ì•ŠìŒ â†’ ì¬ì‹œë„ í•„ìš”")

        except Exception as e:
            print(f"âš ï¸ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ â†’ ì¬ì‹œë„: {e}")

    raise RuntimeError("âŒ í…ŒìŠ¤íŠ¸ì…‹ ìƒì„± ì‹¤íŒ¨: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")

# Testset ìƒì„± ë©”ì¸ ë¡œì§
def generate_testset():
    documents = load_raw_markdown_files()
    if not documents:
        return

    print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ (ì´ {len(documents)}ê°œ)")

    generator = TestsetGenerator(
        llm=generator_llm_wrapper,
        embedding_model=ragas_embeddings
    )

    query_distribution = [
        (SingleHopSpecificQuerySynthesizer(llm=generator_llm_wrapper), 0.8),
        (MultiHopSpecificQuerySynthesizer(llm=generator_llm_wrapper), 0.1),
        (MultiHopAbstractQuerySynthesizer(llm=generator_llm_wrapper), 0.1),
    ]

    print(f"ğŸ“Œ ëª©í‘œ í…ŒìŠ¤íŠ¸ì…‹ í¬ê¸°: {TEST_SIZE}\n")

    df = generate_with_retry(
        generator=generator,
        documents=documents,
        test_size=TEST_SIZE,
        query_distribution=query_distribution,
        max_attempts=5
    )

    os.makedirs(os.path.dirname(OUTPUT_FILE_PATH), exist_ok=True)
    df.to_csv(OUTPUT_FILE_PATH, index=False, encoding="utf-8-sig")

    print(f"\nâœ… CSV ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE_PATH}")
    print(f"   -> ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(df)}")

    if not df.empty:
        print(df[['user_input', 'reference']].head(2))


if __name__ == "__main__":
    os.environ["LANGCHAIN_TRACING_V2"] = "false"
    generate_testset()

import os
import glob
import pandas as pd
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from ragas.testset import TestsetGenerator
from ragas.testset.synthesizers.single_hop.specific import SingleHopSpecificQuerySynthesizer
from ragas.testset.synthesizers.multi_hop.specific import MultiHopSpecificQuerySynthesizer
from ragas.testset.synthesizers.multi_hop.abstract import MultiHopAbstractQuerySynthesizer
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from src.core import get_cached_embedder

load_dotenv()

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR)) 
DATA_DIR = os.path.join(PROJECT_ROOT, "data", "processed_md_originals")
OUTPUT_FILE_PATH = os.path.join(PROJECT_ROOT, "rag_evaluation/dataset", "english_testset.csv")
TEST_SIZE = 10 

generator_llm = ChatOpenAI(model="gpt-5.1", temperature=0, timeout=60)
critic_llm = ChatOpenAI(model="gpt-5.1", temperature=0, timeout=60)

generator_llm_wrapper = LangchainLLMWrapper(generator_llm)
critic_llm_wrapper = LangchainLLMWrapper(critic_llm)
ragas_embeddings = LangchainEmbeddingsWrapper(get_cached_embedder())

def load_raw_markdown_files():
    """
    ì§€ì •ëœ í´ë”(data/processed_md_originals) ë‚´ì˜ ëª¨ë“  .md íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print(f"ğŸ“‚ ì›ë³¸ ë°ì´í„° í´ë” íƒìƒ‰: {DATA_DIR}")
    
    if not os.path.exists(DATA_DIR):
        print(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DATA_DIR}")
        return []

    # í•˜ìœ„ í´ë” í¬í•¨(**) ëª¨ë“  .md íŒŒì¼ ê²€ìƒ‰
    md_files = glob.glob(os.path.join(DATA_DIR, "**/*.md"), recursive=True)
    
    if not md_files:
        print(f"âŒ '{DATA_DIR}' ê²½ë¡œì—ì„œ .md íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    print(f"   -> ì´ {len(md_files)}ê°œì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë°œê²¬")

    documents = []
    for file_path in md_files:
        print(f"   - ë¡œë“œ ì¤‘: {os.path.basename(file_path)}")
        try:
            # UnstructuredMarkdownLoaderëŠ” ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°(í—¤ë” ë“±)ë¥¼ ì˜ íŒŒì•…í•©ë‹ˆë‹¤.
            loader = UnstructuredMarkdownLoader(file_path)
            docs = loader.load()
            # íŒŒì¼ëª… ë©”íƒ€ë°ì´í„° ì¶”ê°€ (Ragasê°€ ë¬¸ì„œë¥¼ êµ¬ë¶„í•˜ëŠ” ë° ì¤‘ìš”)
            for doc in docs:
                doc.metadata['filename'] = os.path.basename(file_path)
            documents.extend(docs)
        except Exception as e:
            print(f"   âš ï¸ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")

    return documents

def generate_testset():
    # 1. ì›ë³¸ ë¬¸ì„œ ë¡œë“œ
    documents = load_raw_markdown_files()
    if not documents: return
    print(f"âœ… ë¡œë“œ ì™„ë£Œ: ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ê°ì²´ ìƒì„±ë¨")

    # 2. Generator ì´ˆê¸°í™”
    generator = TestsetGenerator(
        llm=generator_llm_wrapper,
        embedding_model=ragas_embeddings
    )

    # 3. ì§ˆë¬¸ ë¶„í¬ ì„¤ì •
    query_distribution = [
        (SingleHopSpecificQuerySynthesizer(llm=generator_llm_wrapper), 0.7),
        (MultiHopSpecificQuerySynthesizer(llm=generator_llm_wrapper), 0.1),
        (MultiHopAbstractQuerySynthesizer(llm=generator_llm_wrapper), 0.2),
    ]

    # 4. ë°ì´í„°ì…‹ ìƒì„±
    print(f"3. RAGAS í•©ì„± ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘ (ëª©í‘œ: {TEST_SIZE}ê°œ)...")
    print("   (Ragas ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ì´ ì›ë³¸ MD íŒŒì¼ì„ ìë™ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤)")
    try:
        # transforms=None (ê¸°ë³¸ê°’)ì„ ì‚¬ìš©í•˜ì—¬ Ragasê°€ 
        # HeadlineSplitter -> EmbeddingExtractor ë“±ì˜ í‘œì¤€ ê³¼ì •ì„ ìˆ˜í–‰í•˜ê²Œ í•©ë‹ˆë‹¤.
        testset = generator.generate_with_langchain_docs(
            documents=documents,
            testset_size=TEST_SIZE,
            query_distribution=query_distribution,
            raise_exceptions=False 
        )

        # 5. ì €ì¥
        print("4. ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ, CSV ì €ì¥ ì¤‘...")
        df = testset.to_pandas()
        
        os.makedirs(os.path.dirname(OUTPUT_FILE_PATH), exist_ok=True)
        df.to_csv(OUTPUT_FILE_PATH, index=False, encoding='utf-8-sig') 
        print(f"âœ… ë°ì´í„°ì…‹ ì €ì¥ ì„±ê³µ: {OUTPUT_FILE_PATH}")
        print(f"   -> ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(df)}")
        
        if not df.empty:
            print(df[['user_input', 'reference']].head(2))

    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # LangSmith tracing ë¹„í™œì„±í™”
    os.environ["LANGCHAIN_TRACING_V2"] = "false"
    generate_testset()
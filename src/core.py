import os
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI
from langchain.storage import LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_postgres import PGVector
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever
from langchain.retrievers.document_compressors.base import DocumentCompressorPipeline
from langchain.retrievers.document_compressors.embeddings_filter import EmbeddingsFilter
from langchain_community.document_transformers import EmbeddingsRedundantFilter, LongContextReorder
from util.stopwords import get_korean_stopwords
from src.kiwi_tokenizer import KiwiBM25Tokenizer

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

# OpenAI API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["OPENWEATHER_API_KEY"] = OPENWEATHER_API_KEY
os.environ["DEEPSEEK_API_KEY"] = DEEPSEEK_API_KEY

# llm_default : ConversationSummaryBufferMemory ìš”ì•½, sql_agent
llm_default = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=1,
    api_key=OPENAI_API_KEY
)

# RAGìš© llm (stream option ë¯¸í¬í•¨)
llm_rag = ChatOpenAI(
    model="gpt-4.1",
    temperature=0,
    max_retries=10,
    timeout=120,
    api_key=OPENAI_API_KEY
)

# llm_streaming : ë©”ì¸ ìŠ¤íŠ¸ë¦¬ë° Agentìš© (í† í° ì¶”ì  ì˜µì…˜ í¬í•¨)
llm_streaming = ChatOpenAI(
    model="gpt-4.1", 
    temperature=0, 
    api_key=OPENAI_API_KEY,
    model_kwargs={
        "stream_options": {"include_usage": True}
    }
)

# # --- PostgreDB ì‹±ê¸€í†¤ (ê¸°ì¡´) ---
engine = create_engine(DATABASE_URL)
_db_langchain= None

def get_db_langchain():
    global _db_langchain

    if _db_langchain is not None:
        return _db_langchain

    _db_langchain = SQLDatabase(engine=engine, include_tables=['jeju_accidents'])

    return _db_langchain

# --- Cached Embedder ì‹±ê¸€í†¤ ---
_cached_embedder_instance = None

def get_cached_embedder():
    global _cached_embedder_instance
    if _cached_embedder_instance is not None:
        return _cached_embedder_instance

    print("Initializing CacheBackedEmbeddings...")
    # ìºì‹œë¥¼ ì €ì¥í•  ë¡œì»¬ íŒŒì¼ ì €ì¥ì†Œ ì„¤ì •
    store = LocalFileStore(root_path="./.cache/embeddings")
    
    # ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸ (OpenAI)
    underlying_embeddings = OpenAIEmbeddings()
    
    # ìºì‹œ ì§€ì› ì„ë² ë”© ëª¨ë¸ ìƒì„±
    _cached_embedder_instance = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings, 
        store, 
        namespace="tourism_docs" # ìºì‹œ ID
    )
    return _cached_embedder_instance

# --- PGVector Store ì‹±ê¸€í†¤ ---
_vector_store_instance = None
COLLECTION_NAME = "tourism_docs"

def get_vector_store():
    global _vector_store_instance
    if _vector_store_instance is not None:
        return _vector_store_instance

    print(f"Initializing PGVector connection to collection: {COLLECTION_NAME}...")
    _vector_store_instance = PGVector(
        collection_name=COLLECTION_NAME,
        connection=DATABASE_URL,
        embeddings=get_cached_embedder()
    )
    return _vector_store_instance

# --- BM25 Retriever ì‹±ê¸€í†¤ ---
_bm25_retriever_instance = None
_korean_bm25_tokenizer = None 

def get_korean_bm25_tokenizer():
    """ë¶ˆìš©ì–´ë¥¼ ë¡œë“œí•˜ê³  KiwiBM25Tokenizerë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì‹±ê¸€í†¤ í•¨ìˆ˜"""
    global _korean_bm25_tokenizer
    if _korean_bm25_tokenizer is not None:
        return _korean_bm25_tokenizer

    korean_stopwords = get_korean_stopwords()
    _korean_bm25_tokenizer = KiwiBM25Tokenizer(stop_words=korean_stopwords)
    print(f"âœ… BM25 Tokenizer ì´ˆê¸°í™” ì™„ë£Œ. ë¶ˆìš©ì–´ {len(korean_stopwords)}ê°œ ì ìš©ë¨.")
    return _korean_bm25_tokenizer

def load_documents_from_vectorstore():
    """
    ë²¡í„°ìŠ¤í† ì–´(DB)ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    BM25 ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ë° RAGAS í‰ê°€ìš© ë¬¸ì„œ ìƒì„±ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    print("ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
    
    sql_query = f"""
        SELECT document, cmetadata 
        FROM langchain_pg_embedding
        WHERE collection_id = (
            SELECT uuid FROM langchain_pg_collection WHERE name = '{COLLECTION_NAME}'
        );
    """
    
    documents = []
    try:
        with engine.connect() as connection:
            results = connection.execute(text(sql_query))
            for row in results:
                documents.append(Document(page_content=row[0], metadata=row[1]))
        
        print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ì—ì„œ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        
        if not documents:
            print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë²¡í„°ìŠ¤í† ì–´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return []
        
        return documents
        
    except Exception as e:
        print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

def get_bm25_retriever():
    """
    BM25Retriever ì‹±ê¸€í†¤ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìµœì´ˆ í˜¸ì¶œ ì‹œ load_documents_from_vectorstore()ë¥¼ ì‚¬ìš©í•˜ì—¬
    DB(PGVector)ì— ì €ì¥ëœ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    global _bm25_retriever_instance
    if _bm25_retriever_instance is not None:
        return _bm25_retriever_instance

    print("Initializing BM25Retriever (from PGVector's stored documents)...")
    
    # DBì—ì„œ ë¬¸ì„œ ë¡œë“œ
    splits_from_db = load_documents_from_vectorstore()

    # BM25 ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    if not splits_from_db:
        print("âŒ BM25 Error: DBì— ë¬¸ì„œê°€ ì—†ì–´ 'splits'ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        _bm25_retriever_instance = BM25Retriever.from_documents([], k=2)
    else:
        _bm25_retriever_instance = BM25Retriever.from_documents(
            splits_from_db, 
            k=2,
            custom_tokenizer=get_korean_bm25_tokenizer()
        )
    
    print("BM25Retriever initialization complete.")
    return _bm25_retriever_instance

# --- ìµœì¢… RAG ë¦¬íŠ¸ë¦¬ë²„ ì‹±ê¸€í†¤ ---
_compression_retriever_instance = None
def get_compression_retriever():
    """
    ì••ì¶•/í•„í„°ë§ ê¸°ëŠ¥ì´ í¬í•¨ëœ ìµœì¢… ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì‹±ê¸€í†¤ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global _compression_retriever_instance
    if _compression_retriever_instance is not None:
        return _compression_retriever_instance

    print("Initializing Compression Retriever (Ensemble + Filters)...")
    
    # 1. ì˜ë¯¸, í‚¤ì›Œë“œ ë¦¬íŠ¸ë¦¬ë²„ ê°€ì ¸ì˜¤ê¸°
    vector_retriever = get_vector_store().as_retriever(search_kwargs={"k": 2})
    bm25_retriever = get_bm25_retriever()
    
    # 2. ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.5, 0.5]
    )
    
    # 3. ì••ì¶• í•„í„° ìƒì„± (ì„ë² ë” ì‹±ê¸€í†¤ ì‚¬ìš©)
    cached_embedder = get_cached_embedder()
    
    redundant_filter = EmbeddingsRedundantFilter(
        embeddings=cached_embedder,
        similarity_threshold=0.95
    )
    relevance_filter = EmbeddingsFilter(
        embeddings=cached_embedder,
        similarity_threshold=0.80
    )

    reorder_transformer = LongContextReorder()
    
    # 4. ì••ì¶• íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline_compressor = DocumentCompressorPipeline(
        transformers=[redundant_filter, relevance_filter, reorder_transformer]
    )
    
    # 5. ìµœì¢… ì••ì¶• ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ë° ì €ì¥
    _compression_retriever_instance = ContextualCompressionRetriever(
        base_compressor=pipeline_compressor,
        base_retriever=ensemble_retriever
    )
    
    print("Compression Retriever initialization complete.")
    return _compression_retriever_instance

# --- ë””ë²„ê¹…ìš©: ë¦¬íŠ¸ë¦¬ë²„ ê° ë‹¨ê³„ë³„ ê²°ê³¼ í™•ì¸ í•¨ìˆ˜ ---
def debug_retriever_pipeline(query: str):
    """
    ë¦¬íŠ¸ë¦¬ë²„ íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ë””ë²„ê¹…í•©ë‹ˆë‹¤.
    - Stage 1: BM25 ê²°ê³¼
    - Stage 2: Vector ê²°ê³¼
    - Stage 3: Ensemble ê²°ê³¼
    - Stage 4: í•„í„°ë§ í›„ ìµœì¢… ê²°ê³¼
    """
    print("\n" + "="*60)
    print(f"ğŸ” ë””ë²„ê¹…: ì¿¼ë¦¬ = '{query}'")
    print("="*60)
    
    # Stage 1: BM25
    bm25_ret = get_bm25_retriever()
    bm25_docs = bm25_ret.invoke(query)
    print(f"\n[Stage 1] BM25 ê²€ìƒ‰ ê²°ê³¼: {len(bm25_docs)}ê°œ")
    for i, doc in enumerate(bm25_docs, 1):
        content_preview = doc.page_content[:80].replace('\n', ' ')
        print(f"  {i}. {content_preview}...")
    
    # Stage 2: Vector
    vector_ret = get_vector_store().as_retriever(search_kwargs={"k": 5})
    vector_docs = vector_ret.invoke(query)
    print(f"\n[Stage 2] Vector ê²€ìƒ‰ ê²°ê³¼: {len(vector_docs)}ê°œ")
    for i, doc in enumerate(vector_docs, 1):
        content_preview = doc.page_content[:80].replace('\n', ' ')
        print(f"  {i}. {content_preview}...")
    
    # Stage 3: Ensemble
    ensemble_ret = EnsembleRetriever(
        retrievers=[bm25_ret, vector_ret],
        weights=[0.5, 0.5]
    )
    ensemble_docs = ensemble_ret.invoke(query)
    print(f"\n[Stage 3] Ensemble í†µí•© ê²°ê³¼: {len(ensemble_docs)}ê°œ")
    for i, doc in enumerate(ensemble_docs, 1):
        content_preview = doc.page_content[:80].replace('\n', ' ')
        print(f"  {i}. {content_preview}...")
    
    # Stage 4: Final (Compression + Filters)
    compression_ret = get_compression_retriever()
    final_docs = compression_ret.invoke(query)
    print(f"\n[Stage 4] í•„í„°ë§ ìµœì¢… ê²°ê³¼: {len(final_docs)}ê°œ")
    for i, doc in enumerate(final_docs, 1):
        content_preview = doc.page_content[:100].replace('\n', ' ')
        print(f"  {i}. {content_preview}...")
    
    print("\n" + "="*60)
    return final_docs
import os
from typing import List, Callable
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# LangChain 1.0+ ì´ë™ ëª¨ë“ˆ
from langchain_classic.storage import LocalFileStore
from langchain_community.retrievers import BM25Retriever
from langchain_classic.embeddings import CacheBackedEmbeddings
from langchain_classic.retrievers import (
    ContextualCompressionRetriever, 
    EnsembleRetriever
)
from langchain_classic.retrievers.document_compressors import (
    DocumentCompressorPipeline, 
    EmbeddingsFilter
)
from langchain_community.document_transformers import (
    EmbeddingsRedundantFilter, 
    LongContextReorder
)

# Core ë° Postgres
from langchain_core.documents import Document
from langchain_postgres.vectorstores import PGVector
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from util.stopwords import get_korean_stopwords
from src.kiwi_tokenizer import KiwiBM25Tokenizer
from langchain_cohere import CohereRerank
from langchain_core.prompts import ChatPromptTemplate, load_prompt
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
CORE_DIR = os.path.dirname(CURRENT_DIR)
PROMPT_FILE = os.path.join(CORE_DIR, "prompts", "search_query_translation.yaml")

# API í‚¤ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
if OPENAI_API_KEY:
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
if OPENWEATHER_API_KEY:
    os.environ["OPENWEATHER_API_KEY"] = OPENWEATHER_API_KEY
if DEEPSEEK_API_KEY:
    os.environ["DEEPSEEK_API_KEY"] = DEEPSEEK_API_KEY

# LLM ì„¤ì •
llm_default = ChatOpenAI(
    model="gpt-4.1",
    temperature=1
)

llm_rag = ChatOpenAI(
    model="gpt-5.1",
    temperature=0,
    max_retries=10,
    timeout=120
)

llm_streaming = ChatOpenAI(
    model="gpt-5.1", 
    temperature=0, 
    model_kwargs={
        "stream_options": {"include_usage": True}
    }
)

# --- PostgreDB ì‹±ê¸€í†¤ ---
engine = create_engine(DATABASE_URL or "")
_db_langchain= None

def get_db_langchain():
    global _db_langchain
    if _db_langchain is not None:
        return _db_langchain
    _db_langchain = SQLDatabase(engine=engine, include_tables=['jeju_accidents'])
    return _db_langchain

# --- Cached Embedder ì‹±ê¸€í†¤ ---
_cached_embedder_instance = None

def get_cached_embedder():
    global _cached_embedder_instance
    if _cached_embedder_instance is not None:
        return _cached_embedder_instance

    print("Initializing CacheBackedEmbeddings...")
    store = LocalFileStore(root_path="./.cache/embeddings")
    underlying_embeddings = OpenAIEmbeddings()
    _cached_embedder_instance = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings, 
        store, 
        namespace="tourism_docs"
    )
    return _cached_embedder_instance

# --- PGVector Store ì‹±ê¸€í†¤ ---
_vector_store_instance = None
COLLECTION_NAME = "tourism_docs"

def get_vector_store():
    global _vector_store_instance
    if _vector_store_instance is not None:
        return _vector_store_instance

    print(f"Initializing PGVector connection to collection: {COLLECTION_NAME}...")
    _vector_store_instance = PGVector(
        collection_name=COLLECTION_NAME,
        connection=DATABASE_URL,
        embeddings=get_cached_embedder()
    )
    return _vector_store_instance

# --- BM25 ê´€ë ¨ ìœ í‹¸ë¦¬í‹° ---
_bm25_retriever_instance = None
_korean_bm25_tokenizer = None 

def get_korean_bm25_tokenizer():
    global _korean_bm25_tokenizer
    if _korean_bm25_tokenizer is not None:
        return _korean_bm25_tokenizer

    korean_stopwords = get_korean_stopwords()
    _korean_bm25_tokenizer = KiwiBM25Tokenizer(stop_words=korean_stopwords)
    print(f"âœ… BM25 Tokenizer ì´ˆê¸°í™” ì™„ë£Œ. ë¶ˆìš©ì–´ {len(korean_stopwords)}ê°œ ì ìš©ë¨.")
    return _korean_bm25_tokenizer

def load_documents_from_vectorstore():
    print("ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
    sql_query = f"""
        SELECT document, cmetadata 
        FROM langchain_pg_embedding
        WHERE collection_id = (
            SELECT uuid FROM langchain_pg_collection WHERE name = '{COLLECTION_NAME}'
        );
    """
    documents = []
    try:
        with engine.connect() as connection:
            results = connection.execute(text(sql_query))
            for row in results:
                documents.append(Document(page_content=row[0], metadata=row[1]))
        
        print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ì—ì„œ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        if not documents:
            print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë²¡í„°ìŠ¤í† ì–´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return []
        return documents
    except Exception as e:
        print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

def get_bm25_retriever():
    global _bm25_retriever_instance
    if _bm25_retriever_instance is not None:
        return _bm25_retriever_instance

    print("Initializing BM25Retriever (from PGVector's stored documents)...")
    splits_from_db = load_documents_from_vectorstore()

    if not splits_from_db:
        print("âŒ BM25 Error: DBì— ë¬¸ì„œê°€ ì—†ì–´ 'splits'ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        _bm25_retriever_instance = BM25Retriever.from_documents([], k=5)
    else:
        _bm25_retriever_instance = BM25Retriever.from_documents(
            splits_from_db, 
            k=5,
            preprocess_func=get_korean_bm25_tokenizer()
        )
    print("BM25Retriever initialization complete.")
    return _bm25_retriever_instance


# --- ì¿¼ë¦¬ ì²˜ë¦¬ ì²´ì¸ ---
def get_query_rewrite_chain():
    llm = ChatOpenAI(model="gpt-5.1", temperature=0)
    
    try:
        prompt = load_prompt(PROMPT_FILE)
    except Exception as e:
        print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}")
        from langchain_core.prompts import ChatPromptTemplate
        prompt = ChatPromptTemplate.from_template(
            "Translate the following to natural Korean for search: {question}"
        )
    
    return prompt | llm | StrOutputParser()


# --- ìµœì¢… RAG ë¦¬íŠ¸ë¦¬ë²„ ì‹±ê¸€í†¤ ---
_compression_retriever_instance = None

def get_compression_retriever():
    """
    ì••ì¶•/í•„í„°ë§ ê¸°ëŠ¥ì´ í¬í•¨ëœ ìµœì¢… ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì‹±ê¸€í†¤ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global _compression_retriever_instance
    if _compression_retriever_instance is not None:
        return _compression_retriever_instance

    print("Initializing Compression Retriever (Ensemble + Filters)...")

    # 1. ì˜ë¯¸(Vector) ë¦¬íŠ¸ë¦¬ë²„: ìì—°ì–´ ì¿¼ë¦¬ ì„ í˜¸
    vector_retriever = get_vector_store().as_retriever(search_kwargs={"k": 5})
    
    # 2. í‚¤ì›Œë“œ(BM25) ë¦¬íŠ¸ë¦¬ë²„: í‚¤ì›Œë“œ ì¿¼ë¦¬ ì„ í˜¸(bm25 ë‚´ë¶€ì—ì„œ ë“±ë¡í•œ í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‘ì—… ìˆ˜í–‰)
    raw_bm25_retriever = get_bm25_retriever()
    
    # 3. ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    ensemble_retriever = EnsembleRetriever(
        retrievers=[raw_bm25_retriever, vector_retriever],
        weights=[0.5, 0.5]
    )
    
    # 4. ì••ì¶• í•„í„° ìƒì„±
    cached_embedder = get_cached_embedder()
    
    redundant_filter = EmbeddingsRedundantFilter(
        embeddings=cached_embedder,
        similarity_threshold=0.95
    )
    relevance_filter = EmbeddingsFilter(
        embeddings=cached_embedder,
        similarity_threshold=0.7
    )

    reorder_transformer = LongContextReorder()

    reranker = CohereRerank(
        model="rerank-multilingual-v3.0",
        top_n=5
    )
    
    # 5. ì••ì¶• íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline_compressor = DocumentCompressorPipeline(
        transformers=[redundant_filter, relevance_filter, reranker]
    )
    
    # 6. ìµœì¢… ì••ì¶• ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    _compression_retriever_instance = ContextualCompressionRetriever(
        base_compressor=pipeline_compressor,
        base_retriever=ensemble_retriever
    )
    
    print("Compression Retriever initialization complete.")
    return _compression_retriever_instance

def create_query_processing_chain():
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ â†’ ì˜ì–´/í•œêµ­ì–´ ìì—°ì–´ ìµœì í™” â†’ ë¦¬íŠ¸ë¦¬ë²„ í˜¸ì¶œ
    (í‚¤ì›Œë“œ ë³€í™˜ì€ ì´ì œ ë¦¬íŠ¸ë¦¬ë²„ ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ë¨)
    """
    
    query_rewriter = get_query_rewrite_chain()
    compression_retriever = get_compression_retriever()
    
    processing_chain = (
        {"question": RunnablePassthrough()}
        | query_rewriter                      # 1. ìì—°ì–´ ìµœì í™” (ë²ˆì—­ ë° ì˜ë„ íŒŒì•…)
        | compression_retriever               # 2. ìµœì¢… ë¬¸ì„œ ê²€ìƒ‰
    )
    return processing_chain

# --- ë””ë²„ê¹…ìš© ì²´ì¸ ---
def create_debug_query_chain():
    """
    ê° ë‹¨ê³„ì˜ ì¤‘ê°„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë””ë²„ê¹… ì²´ì¸
    """
    query_rewriter = get_query_rewrite_chain()
    
    def log_optimized_query(query: str) -> str:
        print(f"ğŸ”¹ í•œê¸€ ìì—°ì–´ ì¿¼ë¦¬: {query}")
        return query
    
    def log_retrieval_results(docs):
        print(f"ğŸ”¹ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        for i, doc in enumerate(docs[:5], 1):
            # [ìˆ˜ì •] f-string ë‚´ë¶€ ë°±ìŠ¬ë˜ì‹œ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ë³€ìˆ˜ì— í• ë‹¹ í›„ ì¶œë ¥
            content = doc.page_content[:60].replace('\n', ' ')
            print(f"   {i}. {content}...")
        return docs
    
    compression_retriever = get_compression_retriever()
    
    debug_chain = (
        {"question": RunnablePassthrough()}
        | query_rewriter
        | RunnableLambda(log_optimized_query)
        | compression_retriever
        | RunnableLambda(log_retrieval_results)
    )
    
    return debug_chain

def debug_retriever_pipeline(query: str):
    """
    ë¦¬íŠ¸ë¦¬ë²„ íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹… í•¨ìˆ˜
    """
    print("\n" + "="*60)
    print(f"ğŸ” ë””ë²„ê¹…: ì›ë³¸ ì¿¼ë¦¬ = '{query}'")
    
    rewriter = get_query_rewrite_chain()
    optimized_query = rewriter.invoke({"question": query})
    print(f"ğŸ“ ìµœì í™”ëœ ì¿¼ë¦¬ (ìì—°ì–´): {optimized_query}")
    print("="*60)

    # Stage 1: BM25 (Wrapped)
    print("\n[Stage 1] BM25 ê²€ìƒ‰ (ë‚´ë¶€ì ìœ¼ë¡œ í‚¤ì›Œë“œ ë³€í™˜ ìˆ˜í–‰)")
    raw_bm25_retriever = get_bm25_retriever()
    bm25_docs = raw_bm25_retriever.invoke(optimized_query)
    for i, doc in enumerate(bm25_docs[:3], 1):
        # [ìˆ˜ì •] ì—ëŸ¬ ë°©ì§€ìš© ë³€ìˆ˜ í• ë‹¹
        content = doc.page_content[:80].replace('\n', ' ')
        print(f"  {i}. {content}...")
    
    # Stage 2: Vector
    print("\n[Stage 2] Vector ê²€ìƒ‰ (ìì—°ì–´ ì¿¼ë¦¬ ì‚¬ìš©)")
    vector_ret = get_vector_store().as_retriever(search_kwargs={"k": 5})
    vector_docs = vector_ret.invoke(optimized_query)
    for i, doc in enumerate(vector_docs[:3], 1):
        content = doc.page_content[:80].replace('\n', ' ')
        print(f"  {i}. {content}...")
    
    # Stage 3: Ensemble
    print("\n[Stage 3] Ensemble í†µí•© ê²°ê³¼")
    ensemble_ret = EnsembleRetriever(
        retrievers=[raw_bm25_retriever, vector_ret],
        weights=[0.5, 0.5]
    )
    ensemble_docs = ensemble_ret.invoke(optimized_query)
    for i, doc in enumerate(ensemble_docs[:3], 1):
        content = doc.page_content[:80].replace('\n', ' ')
        print(f"  {i}. {content}...")
    
    # Stage 4: Final
    print("\n[Stage 4] í•„í„°ë§ ìµœì¢… ê²°ê³¼")
    compression_ret = get_compression_retriever()
    final_docs = compression_ret.invoke(optimized_query)
    for i, doc in enumerate(final_docs, 1):
        content = doc.page_content[:100].replace('\n', ' ')
        print(f"  {i}. {content}...")
    
    print("\n" + "="*60)
    return final_docs
import os
import glob
from sqlalchemy import text
from llama_parse import LlamaParse
from llama_index.core import SimpleDirectoryReader
from langchain_core.documents import Document
from langchain_postgres.vectorstores import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter

from src.core import DATABASE_URL, get_cached_embedder, COLLECTION_NAME, engine

# 1. ê²½ë¡œ ì„¤ì •
CURRENT_FILE_PATH = os.path.abspath(__file__)
UTIL_DIR = os.path.dirname(CURRENT_FILE_PATH)
PROJECT_ROOT = os.path.dirname(UTIL_DIR)
DOCS_DIR = os.path.join(PROJECT_ROOT, "data", "tourism_docs")

def get_processed_files(collection_name):
    sql = text(f"""
        SELECT DISTINCT cmetadata->>'source' as source
        FROM langchain_pg_embedding
        WHERE collection_id = (
            SELECT uuid FROM langchain_pg_collection WHERE name = :name
        )
    """)
    processed_files = set()
    try:
        with engine.connect() as conn:
            result = conn.execute(sql, {"name": collection_name})
            for row in result:
                if row[0]: processed_files.add(row[0])
    except Exception as e:
        print(f"âš ï¸ DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (ì²« ì‹¤í–‰ì´ë©´ ë¬´ì‹œ): {e}")
    return processed_files

def delete_existing_file_data(file_path, collection_name):
    sql = text(f"""
        DELETE FROM langchain_pg_embedding
        WHERE collection_id = (
            SELECT uuid FROM langchain_pg_collection WHERE name = :name
        )
        AND cmetadata->>'source' = :path
    """)
    with engine.connect() as conn:
        conn.execute(sql, {"name": collection_name, "path": file_path})
        conn.commit()
    print(f"  ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {os.path.basename(file_path)}")
    

# 1. íŒŒì¼ í•„í„°ë§
all_pdf_files = glob.glob(os.path.join(DOCS_DIR, "*.pdf"))
processed_files = get_processed_files(COLLECTION_NAME)

files_to_process = []
print(f"ğŸ“Š ì´ íŒŒì¼: {len(all_pdf_files)}ê°œ / DB ì €ì¥ë¨: {len(processed_files)}ê°œ")

for pdf_path in all_pdf_files:
    if pdf_path in processed_files:
        print(f"  - [ê±´ë„ˆëœ€] ì´ë¯¸ ìµœì‹ : {os.path.basename(pdf_path)}")
    else:
        print(f"  - [ëŒ€ê¸°ì—´] ì‹ ê·œ/ë³€ê²½: {os.path.basename(pdf_path)}")
        files_to_process.append(pdf_path)

if not files_to_process:
    print("âœ… ì²˜ë¦¬í•  ì‹ ê·œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
    exit()

# 2. LlamaParse ë° Reader ì´ˆê¸°í™”
print(f"\nğŸš€ {len(files_to_process)}ê°œ íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

parser = LlamaParse(
    api_key=os.getenv("LLAMA_CLOUD_API_KEY"),
    result_type="markdown", # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì€ ìœ ì§€ (í‘œ ì²˜ë¦¬ ë“±ì„ ìœ„í•´ ì—¬ì „íˆ ìœ ìš©í•¨)
    num_workers=4,
    verbose=True,
    language="ko"
)

file_extractor = {".pdf": parser}
reader = SimpleDirectoryReader(
    input_files=files_to_process,
    file_extractor=file_extractor
)

# 3. Recursive ë¶„í• ê¸°
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, 
    chunk_overlap=200,
    # ë¶„í•  ìš°ì„ ìˆœìœ„: ë‹¨ë½(\n\n) -> ë¬¸ì¥(\n) -> ë‹¨ì–´( )
    separators=["\n\n", "\n", " ", ""] 
)

# 4. ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
vector_store = PGVector(
    collection_name=COLLECTION_NAME,
    connection=DATABASE_URL,
    embeddings=get_cached_embedder(),
    pre_delete_collection=False
)

# 5. íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬ (ë³‘í•© -> Recursive ë¶„í•  -> ì €ì¥)
for i, docs_in_file in enumerate(reader.iter_data()):
    if not docs_in_file:
        continue

    # â¬‡ï¸ (í•µì‹¬ ìˆ˜ì •) os.path ì•ˆ ì“°ê³  LlamaIndex ë©”íƒ€ë°ì´í„° í™œìš©í•˜ê¸°
    # docs_in_file[0]ì—ëŠ” ì´ë¯¸ íŒŒì¼ ì •ë³´ê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
    first_doc_meta = docs_in_file[0].metadata
    file_path = first_doc_meta.get("file_path", "")
    raw_filename = first_doc_meta.get("file_name", "unknown")
    title = raw_filename.replace(".pdf", "")
    
    print(f"\n--- Processing file: {raw_filename} ({len(docs_in_file)} pages) ---")

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    delete_existing_file_data(file_path, COLLECTION_NAME)

    # í…ìŠ¤íŠ¸ ë³‘í•©
    full_text = "\n\n".join([doc.text for doc in docs_in_file])

    # ë©”íƒ€ë°ì´í„° êµ¬ì„± ë° LangChain Document ìƒì„±
    file_metadata = {
        "source": file_path,
        "title": title,
    }
    
    full_doc = Document(
        page_content=full_text,
        metadata=file_metadata
    )
    
    # ì²­í¬ ë¶„í• 
    final_splits = text_splitter.split_documents([full_doc])
    
    # DB ì €ì¥
    if final_splits:
        vector_store.add_documents(final_splits)
        print(f"  âœ… ì €ì¥ ì™„ë£Œ ({len(final_splits)} chunks) - Title: {title}")
    else:
        print("  âš ï¸ ê²½ê³ : ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
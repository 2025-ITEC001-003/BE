import os
import glob
import tiktoken
from sqlalchemy import text
from llama_parse import LlamaParse
from llama_index.core import SimpleDirectoryReader
from typing import cast, Dict, Any
from langchain_core.documents import Document
from langchain_postgres.vectorstores import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter
from llama_cloud_services.parse.utils import ResultType

from src.core import DATABASE_URL, get_cached_embedder, COLLECTION_NAME, engine

# 1. ê²½ë¡œ ì„¤ì •
CURRENT_FILE_PATH = os.path.abspath(__file__)
UTIL_DIR = os.path.dirname(CURRENT_FILE_PATH)
PROJECT_ROOT = os.path.dirname(UTIL_DIR)
DOCS_DIR = os.path.join(PROJECT_ROOT, "data", "tourism_docs")
PROCESSED_MD_DIR = os.path.join(PROJECT_ROOT, "data", "processed_md_originals")
CHUNKS_DIR = os.path.join(PROJECT_ROOT, "data", "processed_chunks_results")
os.makedirs(PROCESSED_MD_DIR, exist_ok=True)
os.makedirs(CHUNKS_DIR, exist_ok=True)

def get_processed_files(collection_name):
    sql = text(f"""
        SELECT DISTINCT cmetadata->>'source' as source
        FROM langchain_pg_embedding
        WHERE collection_id = (
            SELECT uuid FROM langchain_pg_collection WHERE name = :name
        )
    """)
    processed_files = set()
    try:
        with engine.connect() as conn:
            result = conn.execute(sql, {"name": collection_name})
            for row in result:
                if row[0]: processed_files.add(row[0])
    except Exception as e:
        print(f"âš ï¸ DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (ì²« ì‹¤í–‰ì´ë©´ ë¬´ì‹œ): {e}")
    return processed_files

def delete_existing_file_data(file_path, collection_name):
    sql = text(f"""
        DELETE FROM langchain_pg_embedding
        WHERE collection_id = (
            SELECT uuid FROM langchain_pg_collection WHERE name = :name
        )
        AND cmetadata->>'source' = :path
    """)
    with engine.connect() as conn:
        conn.execute(sql, {"name": collection_name, "path": file_path})
        conn.commit()
    print(f"  ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {os.path.basename(file_path)}")

def token_length(text: str) -> int:
    enc = tiktoken.encoding_for_model("gpt-4o")  # ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë¡œ ì„¤ì •
    return len(enc.encode(text))

def create_korean_text_splitter():
    """
    í•œêµ­ì–´ì— ìµœì í™”ëœ í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„±
    """
    return RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100,
        # í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„ ê³ ë ¤
        separators=[
            "\n\n",              # ë¬¸ë‹¨ êµ¬ë¶„
            "\n",                # ì¤„ë°”ê¿ˆ
            ". ",                # ë§ˆì¹¨í‘œ + ê³µë°± (ì˜ì–´ ë¬¸ì¥)
            "ã€‚ ",               # ì¼ë³¸ì–´/ì¤‘êµ­ì–´ ë§ˆì¹¨í‘œ
            "? ",                # ë¬¼ìŒí‘œ
            "! ",                # ëŠë‚Œí‘œ
            "ï¼› ",               # ì„¸ë¯¸ì½œë¡ 
            "ï¼Œ",                # ì‰¼í‘œ
            " ",                 # ê³µë°±
            "",                  # ìµœí›„ì˜ ìˆ˜ë‹¨
        ],
        # ê¸¸ì´ ê³„ì‚° í•¨ìˆ˜ (í† í° ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)
        length_function=token_length,
        # ë©”íƒ€ë°ì´í„°ì— ì²­í¬ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
        keep_separator=True,
    )

def validate_chunk_quality(chunks: list[Document]) -> tuple[bool, str]:
    """
    ìƒì„±ëœ ì²­í¬ì˜ í’ˆì§ˆì„ ê²€ì¦
    
    Returns:
        (is_valid, message)
    """
    if not chunks:
        return False, "ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
    
    # í‰ê·  ì²­í¬ í¬ê¸° í™•ì¸
    avg_size = sum(token_length(c.page_content) for c in chunks) / len(chunks)
    
    if avg_size < 100:
        return False, f"ì²­í¬ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ (í‰ê· : {avg_size:.0f}í† í°)"
    
    if avg_size > 2000:
        return False, f"ì²­í¬ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ (í‰ê· : {avg_size:.0f}í† í°)"
    
    # ë¹ˆ ì²­í¬ í™•ì¸
    empty_chunks = sum(1 for c in chunks if not c.page_content.strip())
    if empty_chunks > 0:
        return False, f"{empty_chunks}ê°œì˜ ë¹ˆ ì²­í¬ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤"
    
    return True, f"âœ… ì²­í¬ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ (ì´ {len(chunks)}ê°œ, í‰ê·  {avg_size:.0f}í† í°)"
    
# 1. íŒŒì¼ í•„í„°ë§
all_pdf_files = glob.glob(os.path.join(DOCS_DIR, "*.pdf"))
processed_files = get_processed_files(COLLECTION_NAME)

files_to_process = []
print(f"ğŸ“Š ì´ íŒŒì¼: {len(all_pdf_files)}ê°œ / DB ì €ì¥ë¨: {len(processed_files)}ê°œ")

for pdf_path in all_pdf_files:
    if pdf_path in processed_files:
        print(f"  - [ê±´ë„ˆëœ€] ì´ë¯¸ ìµœì‹ : {os.path.basename(pdf_path)}")
    else:
        print(f"  - [ëŒ€ê¸°ì—´] ì‹ ê·œ/ë³€ê²½: {os.path.basename(pdf_path)}")
        files_to_process.append(pdf_path)

if not files_to_process:
    print("âœ… ì²˜ë¦¬í•  ì‹ ê·œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
    exit()

# 2. LlamaParse ë° Reader ì´ˆê¸°í™”
print(f"\nğŸš€ {len(files_to_process)}ê°œ íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

# í…ìŠ¤íŠ¸ ë¶„í• ê¸°
text_splitter = create_korean_text_splitter()

# ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
vector_store = PGVector(
    collection_name=COLLECTION_NAME,
    connection=DATABASE_URL,
    embeddings=get_cached_embedder(),
    pre_delete_collection=False
)

# LlamaParseëŠ” ì‹¤ì œë¡œ í•„ìš”í•  ë•Œë§Œ ì´ˆê¸°í™”(ì²˜ìŒ íŒŒì‹±ì´ í•„ìš”í•œ íŒŒì¼ ë§Œ)
parser = None
file_extractor = None

# íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬: ì´ë¯¸ ì²˜ë¦¬ëœ Markdownì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ LlamaParseë¡œ PDF íŒŒì‹±
for pdf_path in files_to_process:
    raw_filename = os.path.basename(pdf_path)
    title = raw_filename.replace('.pdf', '')
    print(f"\n--- Processing file: {raw_filename} ---")

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    delete_existing_file_data(pdf_path, COLLECTION_NAME)

    # ë¨¼ì € ì´ë¯¸ ë¡œì»¬ì— ì €ì¥ëœ Markdownì´ ìˆëŠ”ì§€ í™•ì¸
    md_save_path = os.path.join(PROCESSED_MD_DIR, f"{title}.md")
    full_text = None

    if os.path.exists(md_save_path):
        try:
            with open(md_save_path, 'r', encoding='utf-8') as f:
                full_text = f.read()
            print(f"  ğŸ“„ ê¸°ì¡´ Markdown ì‚¬ìš©: {md_save_path}")
        except Exception as e:
            print(f"  âŒ ê¸°ì¡´ Markdown ì½ê¸° ì‹¤íŒ¨, íŒŒì‹±ìœ¼ë¡œ ëŒ€ì²´: {e}")

    # Markdownì´ ì—†ìœ¼ë©´ LlamaParseë¡œ PDF íŒŒì‹±
    if not full_text:
        # lazy init parser
        if parser is None:
            parser = LlamaParse(
                api_key=os.getenv("LLAMA_CLOUD_API_KEY", ""),
                parse_mode="parse_page_with_agent",
                model="openai-gpt-4-1-mini",
                high_res_ocr=True,
                adaptive_long_table=True,
                outlined_table_extraction=True,
                precise_bounding_box=True,
                result_type=ResultType.MD,
                num_workers=8,
                verbose=True,
                language="ko"
            )
            file_extractor = cast(Dict[str, Any], {".pdf": parser})

        print(f"  ğŸ” LlamaParseë¡œ PDF íŒŒì‹± ì‹œì‘: {raw_filename}")
        reader = SimpleDirectoryReader(input_files=[pdf_path], file_extractor=file_extractor)
        try:
            docs_iter = reader.iter_data()
            docs_in_file = next(docs_iter, [])
        except Exception as e:
            print(f"  âŒ LlamaParse íŒŒì‹± ì‹¤íŒ¨: {e}")
            docs_in_file = []

        if not docs_in_file:
            print("  âš ï¸ ì¶”ì¶œëœ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ íŒŒì¼ ê±´ë„ˆëœ€.")
            continue

        # LlamaParseë¡œë¶€í„° ë°˜í™˜ëœ ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ë³‘í•©
        parts = []
        for doc in docs_in_file:
            # support different doc types (llama vs langchain)
            text_part = getattr(doc, 'text', None) or getattr(doc, 'page_content', '')
            parts.append(text_part)
        full_text = "\n\n".join(parts)

        # Markdown ì›ë³¸ì„ ë¡œì»¬ì— ì €ì¥
        try:
            with open(md_save_path, 'w', encoding='utf-8') as f:
                f.write(full_text)
            print(f"  ğŸ“ Markdown ì›ë³¸ ì €ì¥ ì™„ë£Œ: {md_save_path}")
        except Exception as e:
            print(f"  âŒ Markdown ì›ë³¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ë©”íƒ€ë°ì´í„° êµ¬ì„± ë° LangChain Document ìƒì„±
    file_metadata = {
        "source": pdf_path,
        "title": title,
    }

    full_doc = Document(
        page_content=full_text or "",
        metadata=file_metadata
    )

    # ì²­í¬ ë¶„í• 
    final_splits = text_splitter.split_documents([full_doc])

    # ì²­í¬ í’ˆì§ˆ ê²€ì¦
    is_valid, message = validate_chunk_quality(final_splits)
    print(f"  {message}")
    if not is_valid:
        print(f"  âŒ ì²­í¬ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨. ì²˜ë¦¬ ì¤‘ë‹¨.")
        continue

    # ì²­í¬ ë¶„í•  ê²°ê³¼ ë¡œì»¬ ì €ì¥
    chunks_save_path = os.path.join(CHUNKS_DIR, f"{title}_chunks.md")
    chunk_separator = "\n\n---\n\n"

    if final_splits:
        try:
            with open(chunks_save_path, "w", encoding="utf-8") as f:
                for chunk_index, chunk in enumerate(final_splits):
                    f.write(f"## CHUNK {chunk_index + 1} (Size: {len(chunk.page_content)} bytes)\n")
                    f.write(chunk.page_content)
                    if chunk_index < len(final_splits) - 1:
                        f.write(chunk_separator)
            print(f"  âœ‚ï¸ ì²­í¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {chunks_save_path} ({len(final_splits)} chunks)")
        except Exception as e:
            print(f"  âŒ ì²­í¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

        # DB ì €ì¥
        vector_store.add_documents(final_splits)
        print(f"  âœ… DB ì €ì¥ ì™„ë£Œ ({len(final_splits)} chunks) - Title: {title}")
    else:
        print("  âš ï¸ ê²½ê³ : ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. DB ì €ì¥ ê±´ë„ˆëœ€.")

print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
import os
import glob
from sqlalchemy import text
from llama_parse import LlamaParse
from llama_index.core import SimpleDirectoryReader
from langchain_core.documents import Document
from langchain_postgres.vectorstores import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter

from src.core import DATABASE_URL, get_cached_embedder, COLLECTION_NAME, engine

# 1. ê²½ë¡œ ì„¤ì •
CURRENT_FILE_PATH = os.path.abspath(__file__)
UTIL_DIR = os.path.dirname(CURRENT_FILE_PATH)
PROJECT_ROOT = os.path.dirname(UTIL_DIR)
DOCS_DIR = os.path.join(PROJECT_ROOT, "data", "tourism_docs")
PROCESSED_MD_DIR = os.path.join(PROJECT_ROOT, "data", "processed_md_originals")
CHUNKS_DIR = os.path.join(PROJECT_ROOT, "data", "processed_chunks_results")
os.makedirs(PROCESSED_MD_DIR, exist_ok=True)
os.makedirs(CHUNKS_DIR, exist_ok=True)

def get_processed_files(collection_name):
    sql = text(f"""
        SELECT DISTINCT cmetadata->>'source' as source
        FROM langchain_pg_embedding
        WHERE collection_id = (
            SELECT uuid FROM langchain_pg_collection WHERE name = :name
        )
    """)
    processed_files = set()
    try:
        with engine.connect() as conn:
            result = conn.execute(sql, {"name": collection_name})
            for row in result:
                if row[0]: processed_files.add(row[0])
    except Exception as e:
        print(f"âš ï¸ DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (ì²« ì‹¤í–‰ì´ë©´ ë¬´ì‹œ): {e}")
    return processed_files

def delete_existing_file_data(file_path, collection_name):
    sql = text(f"""
        DELETE FROM langchain_pg_embedding
        WHERE collection_id = (
            SELECT uuid FROM langchain_pg_collection WHERE name = :name
        )
        AND cmetadata->>'source' = :path
    """)
    with engine.connect() as conn:
        conn.execute(sql, {"name": collection_name, "path": file_path})
        conn.commit()
    print(f"  ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {os.path.basename(file_path)}")
    

# 1. íŒŒì¼ í•„í„°ë§
all_pdf_files = glob.glob(os.path.join(DOCS_DIR, "*.pdf"))
processed_files = get_processed_files(COLLECTION_NAME)

files_to_process = []
print(f"ğŸ“Š ì´ íŒŒì¼: {len(all_pdf_files)}ê°œ / DB ì €ì¥ë¨: {len(processed_files)}ê°œ")

for pdf_path in all_pdf_files:
    if pdf_path in processed_files:
        print(f"  - [ê±´ë„ˆëœ€] ì´ë¯¸ ìµœì‹ : {os.path.basename(pdf_path)}")
    else:
        print(f"  - [ëŒ€ê¸°ì—´] ì‹ ê·œ/ë³€ê²½: {os.path.basename(pdf_path)}")
        files_to_process.append(pdf_path)

if not files_to_process:
    print("âœ… ì²˜ë¦¬í•  ì‹ ê·œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
    exit()

# 2. LlamaParse ë° Reader ì´ˆê¸°í™”
print(f"\nğŸš€ {len(files_to_process)}ê°œ íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

parser = LlamaParse(
    api_key=os.getenv("LLAMA_CLOUD_API_KEY"),
    parse_mode="parse_page_with_agent",
    model="openai-gpt-4-1-mini",
    high_res_ocr=True,
    adaptive_long_table=True,
    outlined_table_extraction=True,
    output_tables_as_HTML=True,
    precise_bounding_box=True,
    result_type="markdown", # LlamaParse ê²°ê³¼ ìœ í˜•
    num_workers=8,
    verbose=True,
    language="ko"
)

file_extractor = {".pdf": parser}
reader = SimpleDirectoryReader(
    input_files=files_to_process,
    file_extractor=file_extractor
)

# 3. Recursive ë¶„í• ê¸°
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)

# 4. ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
vector_store = PGVector(
    collection_name=COLLECTION_NAME,
    connection=DATABASE_URL,
    embeddings=get_cached_embedder(),
    pre_delete_collection=False
)

# 5. íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬ (ë³‘í•© -> Recursive ë¶„í•  -> ì €ì¥ ë° ë¡œì»¬ ì €ì¥)
for i, docs_in_file in enumerate(reader.iter_data()):
    if not docs_in_file:
        continue

    first_doc_meta = docs_in_file[0].metadata
    file_path = first_doc_meta.get("file_path", "")
    raw_filename = first_doc_meta.get("file_name", "unknown")
    title = raw_filename.replace(".pdf", "")
    
    print(f"\n--- Processing file: {raw_filename} ({len(docs_in_file)} pages) ---")

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    delete_existing_file_data(file_path, COLLECTION_NAME)

    # í…ìŠ¤íŠ¸ ë³‘í•© (LlamaParseì˜ Markdown ê²°ê³¼)
    full_text = "\n\n".join([doc.text for doc in docs_in_file])

    # LlamaParse Markdown ì „ì²´ ê²°ê³¼ ë¡œì»¬ ì €ì¥
    md_save_path = os.path.join(PROCESSED_MD_DIR, f"{title}.md")
    try:
        with open(md_save_path, "w", encoding="utf-8") as f:
            f.write(full_text)
        print(f"  ğŸ“ Markdown ì›ë³¸ ì €ì¥ ì™„ë£Œ: {md_save_path}")
    except Exception as e:
        print(f"  âŒ Markdown ì›ë³¸ ì €ì¥ ì‹¤íŒ¨: {e}")


    # ë©”íƒ€ë°ì´í„° êµ¬ì„± ë° LangChain Document ìƒì„±
    file_metadata = {
        "source": file_path,
        "title": title,
    }
    
    full_doc = Document(
        page_content=full_text,
        metadata=file_metadata
    )
    
    # ì²­í¬ ë¶„í• 
    final_splits = text_splitter.split_documents([full_doc])
    
    # ì²­í¬ ë¶„í•  ê²°ê³¼ ë¡œì»¬ ì €ì¥
    chunks_save_path = os.path.join(CHUNKS_DIR, f"{title}_chunks.md")
    chunk_separator = "\n\n---\n\n"
    
    if final_splits:
        try:
            with open(chunks_save_path, "w", encoding="utf-8") as f:
                for chunk_index, chunk in enumerate(final_splits):
                    f.write(f"## CHUNK {chunk_index + 1} (Size: {len(chunk.page_content)} bytes)\n")
                    f.write(chunk.page_content)
                    if chunk_index < len(final_splits) - 1:
                        f.write(chunk_separator)
            print(f"  âœ‚ï¸ ì²­í¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {chunks_save_path} ({len(final_splits)} chunks)")
        except Exception as e:
            print(f"  âŒ ì²­í¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            
        # DB ì €ì¥
        vector_store.add_documents(final_splits)
        print(f"  âœ… DB ì €ì¥ ì™„ë£Œ ({len(final_splits)} chunks) - Title: {title}")
    else:
        print("  âš ï¸ ê²½ê³ : ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. DB ì €ì¥ ê±´ë„ˆëœ€.")

print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")